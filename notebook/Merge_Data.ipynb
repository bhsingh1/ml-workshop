{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JupyterHub Notebook\n",
    "\n",
    "### This notebook server is hosted on the OpenShift platform which provides a separate server for each individual user. The platform takes care of the provisioning of the server and allocating related to storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, install and import required libraries, watermark our file, initialise our Spark Session Builder and initialise our environment with required configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: watermark in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (2.1.0)\r\n",
      "Requirement already satisfied: ipython in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from watermark) (7.16.1)\r\n",
      "Requirement already satisfied: importlib-metadata<3.0 in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from watermark) (1.7.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from importlib-metadata<3.0->watermark) (3.1.0)\r\n",
      "Requirement already satisfied: backcall in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from ipython->watermark) (0.2.0)\r\n",
      "Requirement already satisfied: pickleshare in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from ipython->watermark) (0.7.5)\r\n",
      "Requirement already satisfied: pygments in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from ipython->watermark) (2.6.1)\r\n",
      "Requirement already satisfied: pexpect in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from ipython->watermark) (4.8.0)\r\n",
      "Requirement already satisfied: decorator in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from ipython->watermark) (4.4.2)\r\n",
      "Requirement already satisfied: appnope in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from ipython->watermark) (0.1.0)\r\n",
      "Requirement already satisfied: traitlets>=4.2 in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from ipython->watermark) (4.3.3)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from ipython->watermark) (3.0.5)\r\n",
      "Requirement already satisfied: jedi>=0.10 in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from ipython->watermark) (0.17.1)\r\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from ipython->watermark) (41.2.0)\r\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from jedi>=0.10->ipython->watermark) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->watermark) (0.2.5)\r\n",
      "Requirement already satisfied: ipython-genutils in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from traitlets>=4.2->ipython->watermark) (0.2.0)\r\n",
      "Requirement already satisfied: six in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from traitlets>=4.2->ipython->watermark) (1.15.0)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from pexpect->ipython->watermark) (0.6.0)\r\n",
      "^C\r\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: Minio in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (6.0.0)\r\n",
      "Requirement already satisfied: urllib3 in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from Minio) (1.25.9)\r\n",
      "Requirement already satisfied: python-dateutil in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from Minio) (2.8.1)\r\n",
      "Requirement already satisfied: configparser in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from Minio) (5.0.1)\r\n",
      "Requirement already satisfied: certifi in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from Minio) (2020.6.20)\r\n",
      "Requirement already satisfied: pytz in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from Minio) (2020.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/faisalmasood/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from python-dateutil->Minio) (1.15.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install watermark\n",
    "%pip install Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import from_json, col, to_json, struct\n",
    "import watermark\n",
    "from minio import Minio\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.7.5\n",
      "IPython version      : 7.16.1\n",
      "\n",
      "Compiler    : Clang 11.0.3 (clang-1103.0.32.62)\n",
      "OS          : Darwin\n",
      "Release     : 20.4.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: 94e587d3819dd44872e368d143e20e5d77d4571c\n",
      "\n",
      "sys      : 3.7.5 (default, Jul  7 2020, 21:09:45) \n",
      "[Clang 11.0.3 (clang-1103.0.32.62)]\n",
      "watermark: 2.1.0\n",
      "json     : 2.0.9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -n -v -m -g -iv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sparkSessionBuilder = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Customer Churn ingest Pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'SPARK_CLUSTER'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-dbc15e58f985>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mconf\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhadoop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms3a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mconf\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhadoop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms3a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhadoop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms3a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS3AFileSystem\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmaster\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;34m' + os.environ['\u001b[0m\u001b[0mSPARK_CLUSTER\u001b[0m\u001b[0;34m'] + '\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m7077\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/lib/python3.7/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SPARK_CLUSTER'"
     ]
    }
   ],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \\\n",
    "'--packages \\\n",
    "org.postgresql:postgresql:42.2.10,\\\n",
    "org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5,\\\n",
    "org.apache.kafka:kafka-clients:2.4.0,\\\n",
    "org.apache.spark:spark-streaming_2.11:2.4.5,\\\n",
    "org.apache.hadoop:hadoop-aws:2.7.3 \\\n",
    "--conf spark.jars.ivy=/tmp \\\n",
    "--conf spark.hadoop.fs.s3a.endpoint=http://172.30.226.86:9000 \\\n",
    "--conf spark.hadoop.fs.s3a.access.key=minio \\\n",
    "--conf spark.hadoop.fs.s3a.secret.key=minio123 \\\n",
    "--conf spark.hadoop.fs.s3a.path.style.access=true \\\n",
    "--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\\n",
    "--master spark://' + os.environ['SPARK_CLUSTER'] + ':7077 pyspark-shell '\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Connect to Spark Cluster provided by OpenShift Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark = sparkSessionBuilder.getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "print('Spark context started.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Declare our input data sources, import and combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataFrame_Customer = spark.read\\\n",
    "                .options(delimeter=',', inferSchema='True', header='True') \\\n",
    "                .csv(\"s3a://rawdata/Customer-Churn_P1.csv\")\n",
    "dataFrame_Customer.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dataFrame_Products = spark.read\\\n",
    "#                 .options(delimeter=',', inferSchema='True', header='True') \\\n",
    "#                 .csv(\"s3a://rawdata/Customer-Churn_P2.csv\")\n",
    "# dataFrame_Products.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "srcKafkaBrokers = \"odh-message-bus-kafka-bootstrap:9092\"\n",
    "srcKakaTopic = \"data\"\n",
    "\n",
    "\n",
    "schema = StructType()\\\n",
    "    .add(\"customerID\", IntegerType())\\\n",
    "    .add(\"PhoneService\", StringType())\\\n",
    "    .add(\"MultipleLines\", StringType())\\\n",
    "    .add(\"InternetService\", StringType())\\\n",
    "    .add(\"OnlineSecurity\", StringType())\\\n",
    "    .add(\"OnlineBackup\", StringType())\\\n",
    "    .add(\"DeviceProtection\", StringType())\\\n",
    "    .add(\"TechSupport\", StringType())\\\n",
    "    .add(\"StreamingTV\", StringType())\\\n",
    "    .add(\"StreamingMovies\", StringType())\\\n",
    "    .add(\"PaperlessBilling\", StringType())\\\n",
    "    .add(\"PaymentMethod\", StringType())\\\n",
    "    .add(\"MonthlyCharges\", IntegerType())\\\n",
    "    .add(\"TotalCharges\", IntegerType())\\\n",
    "    .add(\"Churn\", StringType())\n",
    "\n",
    "\n",
    "\n",
    "#Read from JSON Kafka messages into a dataframe\n",
    "dfKafka = spark.read.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", srcKafkaBrokers)\\\n",
    "    .option(\"subscribe\", srcKakaTopic)\\\n",
    "    .option(\"startingOffsets\", \"earliest\")\\\n",
    "    .load()\\\n",
    "    .selectExpr(\"CAST(value AS STRING) as jsonValue\")\\\n",
    "    .rdd.map(lambda row: row[\"jsonValue\"])\n",
    "\n",
    "dfObj = spark.read.schema(schema).json(dfKafka)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataFrom_All = dataFrame_Customer.join(dfObj, \"customerID\", how=\"full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Push prepared data to object storage and stop Spark cluster to save resources\n",
    "###  Note - be sure to change this user_id on the next line to your username (something in the range user1 ... user30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_id = \"user29\"\n",
    "file_location = \"s3a://data/full_data_csv\" + user_id\n",
    "dataFrom_All.repartition(1).write.mode(\"overwrite\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .format(\"csv\").save(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}